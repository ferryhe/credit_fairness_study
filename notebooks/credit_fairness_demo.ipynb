{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Fairness Demo\n\nEnd-to-end walkthrough: generate synthetic credit data, train GLM / NN / ADV_NN, and evaluate fairness at both default and fixed approval thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n\n- Latent score `S*` drives true default risk and is race neutral.\n- Observed score `S` is biased downward for the protected group (`A=1`).\n- Proxy `Z` correlates with race.\n- Goal: compare models on accuracy (ROC/PR) and fairness (EO/DP), including a fixed 2% approval rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom dataclasses import replace\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom src.config import get_default_configs\nfrom src.credit import generate_credit_underwriting_data, train_test_split_df\nfrom src.evaluation.metrics import compute_accuracy_metrics\nfrom src.evaluation.fairness import fairness_metrics, fairness_at_target_rate\nfrom src.models.glm_model import GLMClassifier\nfrom src.models.nn_model import PlainNN, train_plain_nn, predict_proba_plain_nn\nfrom src.models.adv_nn_model import AdvPredictor, train_adv_nn, predict_proba_adv_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / 'src').exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cfg, train_cfg, eval_cfg = get_default_configs()\ndf_full = generate_credit_underwriting_data(sim_cfg)\ndf_train, df_test = train_test_split_df(df_full, test_size=0.2, seed=sim_cfg.seed)\nprint(f\"Train size: {len(df_train)}, Test size: {len(df_test)}\")\ndf_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"S\", \"D\", \"L\"]\nproxy_col = \"Z\"\n\nscaler = StandardScaler()\nX_train = np.concatenate([\n    scaler.fit_transform(df_train[numeric_cols]),\n    df_train[[proxy_col]].to_numpy(),\n], axis=1).astype(np.float32)\ny_train = df_train[\"Y\"].to_numpy(dtype=np.float32)\nA_train = df_train[\"A\"].to_numpy(dtype=np.int64)\n\nX_test = np.concatenate([\n    scaler.transform(df_test[numeric_cols]),\n    df_test[[proxy_col]].to_numpy(),\n], axis=1).astype(np.float32)\ny_test = df_test[\"Y\"].to_numpy(dtype=np.float32)\nA_test = df_test[\"A\"].to_numpy(dtype=np.int64)\nX_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = GLMClassifier().fit(X_train, y_train)\ny_proba_glm = glm.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Plain NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=sim_cfg.seed, stratify=y_train\n)\n\ndef build_loader(X, y, batch_size, shuffle=True):\n    ds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float())\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\ntrain_loader = build_loader(X_tr, y_tr, train_cfg.batch_size, shuffle=True)\nval_loader = build_loader(X_val, y_val, train_cfg.batch_size, shuffle=False)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nplain_nn = PlainNN(input_dim=X_train.shape[1]).to(device)\ntrain_plain_nn(plain_nn, train_loader, val_loader, train_cfg, device)\ny_proba_nn = predict_proba_plain_nn(plain_nn, X_test, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Adversarial NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg_adv = replace(train_cfg, lambda_adv=0.8)\nadv_ds = TensorDataset(\n    torch.from_numpy(X_train).float(),\n    torch.from_numpy(y_train).float(),\n    torch.from_numpy(A_train).long(),\n)\nadv_loader = DataLoader(adv_ds, batch_size=train_cfg.batch_size, shuffle=True)\nadv_model = AdvPredictor(input_dim=X_train.shape[1]).to(device)\ntrain_adv_nn(adv_model, adv_loader, train_cfg_adv, device=device)\ny_proba_adv = predict_proba_adv_nn(adv_model, X_test, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics at threshold 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, y_true, y_proba, A_true):\n    acc = compute_accuracy_metrics(y_true, y_proba)\n    fair = fairness_metrics(y_true, y_proba, A_true, threshold=eval_cfg.threshold)\n    return {\"model\": model, **acc, **fair}\n\nsummary_default = pd.DataFrame([\n    summarize(\"GLM\", y_test, y_proba_glm, A_test),\n    summarize(\"NN\", y_test, y_proba_nn, A_test),\n    summarize(\"ADV_NN\", y_test, y_proba_adv, A_test),\n])\nsummary_default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fairness at 2% approval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_RATE = 0.02\n\ndef summarize_fixed(model, y_true, y_proba, A_true):\n    fair = fairness_at_target_rate(y_true, y_proba, A_true, TARGET_RATE)\n    acc = compute_accuracy_metrics(y_true, y_proba)\n    return {\"model\": model, **acc, **fair}\n\nsummary_fixed = pd.DataFrame([\n    summarize_fixed(\"GLM\", y_test, y_proba_glm, A_test),\n    summarize_fixed(\"NN\", y_test, y_proba_nn, A_test),\n    summarize_fixed(\"ADV_NN\", y_test, y_proba_adv, A_test),\n])\nsummary_fixed[[\"model\", \"roc_auc\", \"eo_gap_tpr\", \"eo_gap_fpr\", \"dp_diff\", \"dp_ratio\", \"threshold\", \"actual_rate\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\nfor ax, group in zip(axes, [0, 1]):\n    ax.hist(y_proba_glm[A_test == group], bins=40, alpha=0.5, label=f\"GLM A={group}\")\n    ax.hist(y_proba_adv[A_test == group], bins=40, alpha=0.5, label=f\"ADV A={group}\")\n    ax.set_title(f\"Predicted score distribution (A={group})\")\n    ax.set_xlabel(\"Predicted probability\")\n    ax.legend()\nplt.tight_layout()\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness frontier (EO gap vs ROC AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frontier = pd.DataFrame({\n    \"model\": summary_fixed[\"model\"],\n    \"roc_auc\": summary_fixed[\"roc_auc\"],\n    \"eo_gap_tpr\": summary_fixed[\"eo_gap_tpr\"],\n})\nplt.figure(figsize=(6, 4))\nfor _, row in df_frontier.iterrows():\n    plt.scatter(row[\"eo_gap_tpr\"], row[\"roc_auc\"], label=row[\"model\"])\n    plt.annotate(row[\"model\"], (row[\"eo_gap_tpr\"], row[\"roc_auc\"]), xytext=(5,5), textcoords=\"offset points\")\nplt.xlabel(\"EO TPR gap\")\nplt.ylabel(\"ROC AUC\")\nplt.title(\"Fairness frontier (2% approval)\")\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n    \"### Key insights\n\"\n    f\"- GLM highest ROC AUC ({summary_fixed.loc[summary_fixed.model=='GLM', 'roc_auc'].iat[0]:.3f}) but EO gap {summary_fixed.loc[summary_fixed.model=='GLM', 'eo_gap_tpr'].iat[0]:.3f}, DP ratio {summary_fixed.loc[summary_fixed.model=='GLM', 'dp_ratio'].iat[0]:.2f} at 2% approval.\n\"\n    f\"- Plain NN slightly less accurate ({summary_fixed.loc[summary_fixed.model=='NN', 'roc_auc'].iat[0]:.3f}) with modest fairness gains.\n\"\n    f\"- ADV_NN maintains accuracy ({summary_fixed.loc[summary_fixed.model=='ADV_NN', 'roc_auc'].iat[0]:.3f}) while reducing EO gap to {summary_fixed.loc[summary_fixed.model=='ADV_NN', 'eo_gap_tpr'].iat[0]:.3f} and DP ratio to {summary_fixed.loc[summary_fixed.model=='ADV_NN', 'dp_ratio'].iat[0]:.2f}.\"\n)\nprint(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
