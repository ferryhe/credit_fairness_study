# Auto bias sweep run — 20251113_191359

This folder captures the latest execution of `python -m src.experiments.auto.bias_sweep` so we can archive the bias-strength sweep alongside a quick narrative about why it was run and what we observed.

## Why this run
- Goal: re-evaluate how GLM, NN, and ADV_NN respond as we amplify synthetic bias (the `bias_strength` knob from the generator) to ensure the adversarial penalty is still reducing fairness gaps without devastating ROC AUC.
- Triggered as a follow-up to the baseline work; the run also refreshes the shared bias-sweep CSV (archived here as `auto_bias_sweep_metrics.csv`) used across experiments.

## Settings
- `bias_strength` sweep: 0.0, 0.25, 0.5, 1.0, 2.0.
- Models trained: `GLM`, `NN`, and `ADV_NN` (with the same training/evaluation configs as in the baseline run directory).
- Metrics logged: ROC AUC, EO gaps (TPR/FPR), DP fixed-rate ratio/diff.

## Key results
- ROC AUC stays between 0.75–0.81; ADV_NN slightly improves over NN at high bias_strength (0.814 vs. 0.772 at `bias_strength=2.0`), showing the adversarial objective is preserving accuracy.
- GLM fairs best on fairness gaps, but the NN sees EO gaps and DP ratios drift upward as bias increases, especially jumping to DP ratio ~1.19 at `bias_strength=0.5` before settling.
- ADV_NN keeps TPR/FPR EO gaps around 0.06–0.08 and DP ratios in the 1.13–1.16 band even at `bias_strength=2.0`, documenting the trade-off frontier saved in `auto_bias_sweep_metrics.csv`.
- Refer to `auto_bias_sweep_metrics.csv` in this folder for the raw numbers per bias strength/model.

Shared artifacts (copied here):

- `metrics/auto_bias_sweep_metrics.csv`: per-model metrics generated by this sweep.

## Metrics snapshot (`auto_bias_sweep_metrics.csv`)

| bias_strength | model_name | roc_auc | eo_gap_tpr | eo_gap_fpr | dp_ratio_fixed_2pct | dp_diff | dp_ratio |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0.0 | GLM | 0.761078658087319 | 0.058600123321128295 | 0.1617473395359632 | 2.9755950937068247 | 0.09251993667202096 | 1.1075504357007262 |
| 0.0 | NN | 0.8089133969172507 | 0.07885095450670665 | 0.27713479911989103 | 2.23900803010266 | 0.1432495530949247 | 1.1723743014952788 |
| 0.0 | ADV_NN | 0.8018208885039976 | 0.06626433784122321 | 0.24964106204677683 | 2.3149226910088743 | 0.12235728008268154 | 1.1412601799665996 |
| 0.25 | GLM | 0.7592239838753555 | 0.05999385154412484 | 0.15952943291216437 | 3.923048257872182 | 0.09312092062644628 | 1.1082191794557652 |
| 0.25 | NN | 0.7992740644192453 | 0.06895164720535674 | 0.25804737753446605 | 1.6813063962309351 | 0.1286887505759292 | 1.1512865239182686 |
| 0.25 | ADV_NN | 0.8088180377615142 | 0.05940570011141133 | 0.2255671902268761 | 2.276654234896316 | 0.11491901683140016 | 1.134292765332878 |
| 0.5 | GLM | 0.7580489726629616 | 0.06202606874888894 | 0.1565060490430975 | 4.37139663020043 | 0.09383334047921277 | 1.1091374663051967 |
| 0.5 | NN | 0.8060504877369123 | 0.08830350139308285 | 0.2928416054026135 | 1.7696670243540207 | 0.15461355902100482 | 1.1884996227670674 |
| 0.5 | ADV_NN | 0.8140696217533996 | 0.06167270854148421 | 0.20005244703532044 | 2.8769020557729332 | 0.11411912079399178 | 1.1377739729539849 |
| 1.0 | GLM | 0.7568794044465863 | 0.06708417107022446 | 0.15510499646181608 | 5.833402192140374 | 0.09708303427938136 | 1.1126837534042102 |
| 1.0 | NN | 0.8111013923788594 | 0.07692692196447848 | 0.25951043366075754 | 3.184592115213889 | 0.14058086575936557 | 1.172792184971596 |
| 1.0 | ADV_NN | 0.8068461434718965 | 0.07325761971448408 | 0.24864217924826681 | 2.8769020557729332 | 0.1346166226785792 | 1.163977610095313 |
| 2.0 | GLM | 0.7551537612578857 | 0.07487135470782214 | 0.1748903845585006 | 7.917424665887495 | 0.10655083793291253 | 1.123587819456134 |
| 2.0 | NN | 0.7728144690060331 | 0.0 | 0.0 | 2.9755950937068247 | 0.0 | 1.0 |
| 2.0 | ADV_NN | 0.8059953996366853 | 0.060336064741832396 | 0.2351140296388322 | 3.352423056727137 | 0.11516186611789858 | 1.1324086329617669 |

## NN bias-strength=2.0 diagnostics

- Created with `python -m src.experiments.auto.auto_fairness_diagnostics` after regenerating the auto dataset at `bias_strength=2.0`.
- Diagnostics folder: `diagnostics/nn_bias_strength_2_diagnostics/` (score distributions, ROC curves, TPR/FPR vs threshold, confusion matrices, NN output histogram).

### Diagnostic summary

- **Are group-level score distributions identical?** No—the majority group averages ~0.7099 while the minority group averages ~0.8077 (difference ≈0.0978), so the NN still scores minorities higher before thresholding.
- **Does the NN collapse to predicting a constant?** No—predicted probabilities have a standard deviation of ~0.171, so the model produces a spread of scores.
- **Does thresholding cause EO gap = 0?** No—the EO gaps at threshold 0.5 remain substantial (TPR gap ≈0.0808, FPR gap ≈0.3097), so the fairness imbalance persists despite the fixed threshold.
