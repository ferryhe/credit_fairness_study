# Auto results directory

Each auto pricing run lives under `results/auto/<run-id>/`, where `<run-id>` matches the timestamp from `python -m src.experiments.auto.baseline` (format `YYYYMMDD_HHMMSS`).  
Inside each run folder:

- `baseline_results.csv` - GLM/NN/ADV_NN metrics (ROC AUC, EO/DP gaps, fixed-rate fairness, etc.).
- `debug_auto_distributions/` - histogram snapshots produced during the simulation run (e.g., `auto_annual_mileage.png`, `auto_premium_charged.png`).
- `README.md` - narrative summary for that particular run (reproduce notes, observations, next steps).
- `fairness_accuracy_frontier.png` - optional EO-gap vs ROC AUC chart produced by `python -m src.experiments.auto.fairness_frontier`.

To add a new auto run:

1. Run `python -m src.experiments.auto.full_pipeline` to replay the baseline, sanity checks, bias sweep, and fairness frontier plot.
2. The pipeline creates `results/auto/<run-id>/` with the baseline metrics + histograms; refresh `README.md` with observations and any relevant links to shared artifacts (`auto_bias_sweep_metrics.csv`, `fairness_accuracy_frontier.png`).
3. Append the run to the log below to keep the history complete.

| Run ID | Description |
| --- | --- |
| 20251113_180630 | Initial auto baseline run with the new generator; documents default bias strengths and debug histograms. |
| 20251113_180642 | Repeat baseline to capture stabilized draws and updated diagnostics. |
| 20251113_183405 | Auto sanity checks covering measurement bias vs unbiased (`bias_strength`) and proxy ablation; GLM + ADV-NN metrics compared. |
| 20251113_191359 | Dedicated bias sweep capture; folder holds the metrics CSV plus a run-level README explaining the bias_strength grid, reasons for the run, and the fairness/accuracy takeaways. |
| 20251113_194731 | Re-ran the bias sweep after refactoring `compute_fairness_metrics`; the NN row at `bias_strength=2.0` now logs EO gaps that agree with the diagnostics. |

Each run directory now keeps structured subfolders:
- `metrics/` for aggregated CSV exports (e.g., the bias sweep table).
- `diagnostics/` for follow-up plots or investigation bundles (like the NN score/distribution plots produced after the sweep).

## Bias sweep insights

- The latest `python -m src.experiments.auto.bias_sweep` sweep lives in `results/auto/auto_bias_sweep_metrics.csv`; ROC AUC across GLM/NN/ADV_NN hovers between 0.75 and 0.81, with the adversarial model defending against performance drops when `bias_strength` increases to 2.0.
- As `bias_strength` climbs, unconstrained NN metrics show sharply increasing FPR gaps and DP ratios falling toward 1.0, while ADV_NN maintains smaller EO gaps (~0.06-0.08) and DP ratios around 1.13-1.16, documenting the trade-offs you can explore in future sweeps.
- Keep this section synchronized with the CSV: rerun the bias sweep, replace the CSV if it changes, and briefly describe any emerging patterns here so downstream readers can quickly understand the latest bias/fairness signals.
- The latest runs are archived under `results/auto/20251113_191359/` (bias sweep + diagnostics) and `results/auto/20251113_194731/` (fairness helper rerun); each folder contains a README plus the `metrics/auto_bias_sweep_metrics.csv` copy for easy reference.
- Plot outputs from the latest sweep are available under `results/auto/bias_sweep_plots/20251113_195813/`; they visualize ROC AUC, EO gaps (TPR/FPR), and fixed-rate DP ratio versus `bias_strength`, and the script that generates them is `src/experiments/auto/plot_bias_sweep_curves.py`. Each rerun creates a timestamped subfolder so you can compare different sweeps side by side.

Shared artifacts:

- `results/auto/auto_bias_sweep_metrics.csv` - aggregated metrics from `python -m src.experiments.auto.bias_sweep`.
- `results/auto/fairness_accuracy_frontier.png` - frontier plot generated by `python -m src.experiments.auto.fairness_frontier` (also copied into the latest baseline folder for easy reference).
