# Auto results directory

Each auto pricing run lives under `results/auto/<run-id>/`, where `<run-id>` matches the timestamp from `python -m src.experiments.auto.baseline` (format `YYYYMMDD_HHMMSS`).  
Inside each run folder:

- `baseline_results.csv` – GLM/NN/ADV_NN metrics (ROC AUC, EO/DP gaps, fixed-rate fairness, etc.).
- `debug_auto_distributions/` – histogram snapshots produced during the simulation run (e.g., `auto_annual_mileage.png`, `auto_premium_charged.png`).
- `README.md` – narrative summary for that particular run (reproduce notes, observations, next steps).
- `fairness_accuracy_frontier.png` – optional EO-gap vs ROC AUC chart produced by `python -m src.experiments.auto.fairness_frontier`.

To add a new auto run:

1. Run `python -m src.experiments.auto.full_pipeline` to replay the baseline, sanity checks, bias sweep, and fairness frontier plot.
2. The pipeline creates `results/auto/<run-id>/` with the baseline metrics + histograms; refresh `README.md` with observations and any relevant links to shared artifacts (`auto_bias_sweep_metrics.csv`, `fairness_accuracy_frontier.png`).
3. Append the run to the log below to keep the history complete.

| Run ID | Description |
| --- | --- |
| 20251113_180630 | Initial auto baseline run with the new generator; documents default bias strengths and debug histograms. |
| 20251113_180642 | Repeat baseline to capture stabilized draws and updated diagnostics. |
| 20251113_183405 | Auto sanity checks covering measurement bias vs unbiased (`bias_strength`) and proxy ablation; GLM + ADV-NN metrics compared. |

Shared artifacts:

- `results/auto/auto_bias_sweep_metrics.csv` – aggregated metrics from `python -m src.experiments.auto.bias_sweep`.
- `results/auto/fairness_accuracy_frontier.png` – frontier plot generated by `python -m src.experiments.auto.fairness_frontier` (also copied into the latest baseline folder for easy reference).
