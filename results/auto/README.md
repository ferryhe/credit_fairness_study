# Auto results directory

Each auto pricing run lives under `results/auto/<run-id>/`, where `<run-id>` matches the timestamp from `python -m src.experiments.auto.baseline` (format `YYYYMMDD_HHMMSS`).  
Inside each run folder:

- `baseline_results.csv` - GLM/NN/ADV_NN metrics (ROC AUC, EO/DP gaps, fixed-rate fairness, etc.).
- `debug_auto_distributions/` - histogram snapshots produced during the simulation run (e.g., `auto_annual_mileage.png`, `auto_premium_charged.png`).
- `README.md` - narrative summary for that particular run (reproduce notes, observations, next steps).
- `fairness_accuracy_frontier.png` - optional EO-gap vs ROC AUC chart produced by `python -m src.experiments.auto.fairness_frontier`.

To add a new auto run:

1. Run `python -m src.experiments.auto.full_pipeline` to replay the baseline, sanity checks, bias sweep, and fairness frontier plot.
2. The pipeline creates `results/auto/<run-id>/` with the baseline metrics + histograms; refresh `README.md` with observations and any relevant links to shared artifacts (`auto_bias_sweep_metrics.csv`, `fairness_accuracy_frontier.png`).
3. Append the run to the log below to keep the history complete.

| Run ID | Description |
| --- | --- |
| 20251113_180630 | Initial auto baseline run with the new generator; documents default bias strengths and debug histograms. |
| 20251113_180642 | Repeat baseline to capture stabilized draws and updated diagnostics. |
| 20251113_183405 | Auto sanity checks covering measurement bias vs unbiased (`bias_strength`) and proxy ablation; GLM + ADV-NN metrics compared. |
| 20251113_191359 | Dedicated bias sweep capture; folder holds the metrics CSV plus a run-level README explaining the bias_strength grid, reasons for the run, and the fairness/accuracy takeaways. |

## Bias sweep insights

- The latest `python -m src.experiments.auto.bias_sweep` sweep lives in `results/auto/auto_bias_sweep_metrics.csv`; ROC AUC across GLM/NN/ADV_NN hovers between 0.75 and 0.81, with the adversarial model defending against performance drops when `bias_strength` increases to 2.0.
- As `bias_strength` climbs, unconstrained NN metrics show sharply increasing FPR gaps and DP ratios falling toward 1.0, while ADV_NN maintains smaller EO gaps (~0.06-0.08) and DP ratios around 1.13-1.16, documenting the trade-offs you can explore in future sweeps.
- Keep this section synchronized with the CSV: rerun the bias sweep, replace the CSV if it changes, and briefly describe any emerging patterns here so downstream readers can quickly understand the latest bias/fairness signals.
- The most recent sweep is archived under `results/auto/20251113_191359/` with its own README and a copy of this CSV if you need per-run context when revisiting the logged metrics.

Shared artifacts:

- `results/auto/auto_bias_sweep_metrics.csv` - aggregated metrics from `python -m src.experiments.auto.bias_sweep`.
- `results/auto/fairness_accuracy_frontier.png` - frontier plot generated by `python -m src.experiments.auto.fairness_frontier` (also copied into the latest baseline folder for easy reference).
